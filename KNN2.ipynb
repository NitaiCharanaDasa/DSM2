{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "274f6b4b-26ba-4a4c-b8b9-43704d41e21b",
   "metadata": {},
   "source": [
    "# KNN 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a13870cb-8bdd-41e2-8bbc-e6d0e163fb52",
   "metadata": {},
   "source": [
    "Answers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ea074d4-0cc2-4359-9fcc-ec4ba7209261",
   "metadata": {},
   "source": [
    "1. **Difference Between Euclidean and Manhattan Distance:**\n",
    "   - *Euclidean Distance:* Measures straight-line distance between two points.\n",
    "   - *Manhattan Distance:* Sum of absolute differences between corresponding coordinates.\n",
    "   - The main difference lies in the path taken to reach the destination. Euclidean considers the shortest path, while Manhattan considers only horizontal and vertical movements. The choice depends on the nature of the data.\n",
    "\n",
    "2. **Choosing Optimal Value of k:**\n",
    "   - Optimal k is typically chosen through cross-validation, evaluating performance metrics for different k values.\n",
    "   - Techniques include grid search, random search, or more sophisticated methods like Bayesian optimization.\n",
    "\n",
    "3. **Impact of Distance Metric Choice:**\n",
    "   - The choice of distance metric affects how \"closeness\" between points is measured.\n",
    "   - Euclidean distance is sensitive to magnitudes, while Manhattan is less sensitive.\n",
    "   - Choose based on the characteristics of the data; for example, use Manhattan if features have different scales.\n",
    "\n",
    "4. **Common Hyperparameters in KNN:**\n",
    "   - *k:* Number of neighbors to consider.\n",
    "   - *Distance metric:* Measures the similarity between instances.\n",
    "   - *Weights:* Uniform or distance-based weighting of neighbors.\n",
    "   - Hyperparameters can be tuned through techniques like grid search or random search to find optimal values.\n",
    "\n",
    "5. **Effect of Training Set Size:**\n",
    "   - Larger training sets generally lead to better generalization.\n",
    "   - However, computational complexity increases with more data.\n",
    "   - Techniques like cross-validation can help determine an appropriate balance for the dataset size.\n",
    "\n",
    "6. **Potential Drawbacks of KNN:**\n",
    "   - *Sensitive to outliers:* Outliers can significantly impact predictions.\n",
    "   - *Computational cost:* For large datasets, the algorithm can be slow.\n",
    "   - *Curse of dimensionality:* Performance can degrade as the number of features increases.\n",
    "   - To overcome these, consider outlier removal, dimensionality reduction techniques, and optimization of hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d10ad5c-1704-4f29-9994-bfe3a0c03ea5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
